{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 17:47:14,593\tINFO worker.py:1553 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pprint\n",
    "import ray\n",
    "ray.init()\n",
    "checkpoint_path = \"model_checkpoint/best_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'extra_python_environs_for_driver': {}, 'extra_python_environs_for_worker': {}, 'num_gpus': 1, 'num_cpus_per_worker': 1, 'num_gpus_per_worker': 0, '_fake_gpus': False, 'num_trainer_workers': 0, 'num_gpus_per_trainer_worker': 0, 'num_cpus_per_trainer_worker': 1, 'custom_resources_per_worker': {}, 'placement_strategy': 'PACK', 'eager_tracing': False, 'eager_max_retraces': 20, 'tf_session_args': {'intra_op_parallelism_threads': 2, 'inter_op_parallelism_threads': 2, 'gpu_options': {'allow_growth': True}, 'log_device_placement': False, 'device_count': {'CPU': 1}, 'allow_soft_placement': True}, 'local_tf_session_args': {'intra_op_parallelism_threads': 8, 'inter_op_parallelism_threads': 8}, 'env': None, 'env_config': {}, 'observation_space': None, 'action_space': None, 'env_task_fn': None, 'render_env': False, 'clip_rewards': None, 'normalize_actions': True, 'clip_actions': False, 'disable_env_checking': False, 'is_atari': None, 'auto_wrap_old_gym_envs': True, 'num_envs_per_worker': 1, 'sample_collector': <class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>, 'sample_async': False, 'enable_connectors': True, 'rollout_fragment_length': 50, 'batch_mode': 'truncate_episodes', 'remote_worker_envs': False, 'remote_env_batch_wait_ms': 0, 'validate_workers_after_construction': True, 'ignore_worker_failures': False, 'recreate_failed_workers': False, 'restart_failed_sub_environments': False, 'num_consecutive_worker_failures_tolerance': 100, 'preprocessor_pref': 'deepmind', 'observation_filter': 'NoFilter', 'synchronize_filters': True, 'compress_observations': False, 'enable_tf1_exec_eagerly': False, 'sampler_perf_stats_ema_coef': None, 'worker_health_probe_timeout_s': 60, 'worker_restore_timeout_s': 1800, 'gamma': 0.99, 'lr': 0.0003, 'train_batch_size': 512, 'model': {'_disable_preprocessor_api': False, '_disable_action_flattening': False, 'fcnet_hiddens': [256, 256], 'fcnet_activation': 'tanh', 'conv_filters': None, 'conv_activation': 'relu', 'post_fcnet_hiddens': [], 'post_fcnet_activation': 'relu', 'free_log_std': False, 'no_final_linear': False, 'vf_share_layers': True, 'use_lstm': False, 'max_seq_len': 20, 'lstm_cell_size': 256, 'lstm_use_prev_action': False, 'lstm_use_prev_reward': False, '_time_major': False, 'use_attention': False, 'attention_num_transformer_units': 1, 'attention_dim': 64, 'attention_num_heads': 1, 'attention_head_dim': 32, 'attention_memory_inference': 50, 'attention_memory_training': 50, 'attention_position_wise_mlp_dim': 32, 'attention_init_gru_gate_bias': 2.0, 'attention_use_n_prev_actions': 0, 'attention_use_n_prev_rewards': 0, 'framestack': True, 'dim': 84, 'grayscale': False, 'zero_mean': True, 'custom_model': None, 'custom_model_config': {}, 'custom_action_dist': None, 'custom_preprocessor': None, 'lstm_use_prev_action_reward': -1, '_use_default_native_models': -1}, 'optimizer': {}, 'max_requests_in_flight_per_sampler_worker': 2, 'rl_trainer_class': None, '_enable_rl_trainer_api': False, '_rl_trainer_hps': RLTrainerHPs(), 'explore': True, 'exploration_config': {'type': 'StochasticSampling'}, 'policies': {'default_policy': <ray.rllib.policy.policy.PolicySpec object at 0x7f8335c734c0>}, 'policy_states_are_swappable': False, 'input_config': {}, 'actions_in_input_normalized': False, 'postprocess_inputs': False, 'shuffle_buffer_size': 0, 'output': None, 'output_config': {}, 'output_compress_columns': ['obs', 'new_obs'], 'output_max_file_size': 67108864, 'offline_sampling': False, 'evaluation_interval': None, 'evaluation_duration': 10, 'evaluation_duration_unit': 'episodes', 'evaluation_sample_timeout_s': 180.0, 'evaluation_parallel_to_training': False, 'evaluation_config': None, 'off_policy_estimation_methods': {}, 'ope_split_batch_by_episode': True, 'evaluation_num_workers': 0, 'always_attach_evaluation_results': False, 'enable_async_evaluation': False, 'in_evaluation': False, 'sync_filters_on_rollout_workers_timeout_s': 60.0, 'keep_per_episode_custom_metrics': False, 'metrics_episode_collection_timeout_s': 60.0, 'metrics_num_episodes_for_smoothing': 100, 'min_time_s_per_iteration': 10, 'min_train_timesteps_per_iteration': 0, 'min_sample_timesteps_per_iteration': 0, 'export_native_model_files': False, 'checkpoint_trainable_policies_only': False, 'logger_creator': None, 'logger_config': None, 'log_level': 'WARN', 'log_sys_usage': True, 'fake_sampler': False, 'seed': None, 'worker_cls': None, 'rl_module_class': None, '_enable_rl_module_api': False, '_tf_policy_handles_more_than_one_loss': True, '_disable_preprocessor_api': False, '_disable_action_flattening': False, '_disable_execution_plan_api': True, 'simple_optimizer': -1, 'replay_sequence_length': None, 'horizon': -1, 'soft_horizon': -1, 'no_done_at_end': -1, 'vtrace': True, 'vtrace_clip_rho_threshold': 1.0, 'vtrace_clip_pg_rho_threshold': 1.0, 'vtrace_drop_last_ts': True, 'num_multi_gpu_tower_stacks': 1, 'minibatch_buffer_size': 1, 'num_sgd_iter': 1, 'replay_proportion': 0.0, 'replay_buffer_num_slots': 0, 'learner_queue_size': 16, 'learner_queue_timeout': 300, 'max_requests_in_flight_per_aggregator_worker': 2, 'timeout_s_sampler_manager': 0.0, 'timeout_s_aggregator_manager': 0.0, 'broadcast_interval': 1, 'num_aggregation_workers': 0, 'grad_clip': 40.0, 'opt_type': 'adam', 'lr_schedule': None, 'decay': 0.99, 'momentum': 0.0, 'epsilon': 0.1, 'vf_loss_coeff': 0.5, 'entropy_coeff': 0.01, 'entropy_coeff_schedule': None, '_separate_vf_optimizer': False, '_lr_vf': 0.0005, 'after_train_step': None, 'num_data_loader_buffers': -1, 'input': 'sampler', 'multiagent': {'policies': {'default_policy': (None, None, None, None)}, 'policy_mapping_fn': <function AlgorithmConfig.__init__.<locals>.<lambda> at 0x7f825023c790>, 'policies_to_train': None, 'policy_map_capacity': 100, 'policy_map_cache': -1, 'count_steps_by': 'env_steps', 'observation_fn': None}, 'callbacks': <class 'ray.rllib.algorithms.callbacks.DefaultCallbacks'>, 'create_env_on_driver': False, 'custom_eval_function': None, 'framework': 'torch', 'num_cpus_for_driver': 1, 'num_workers': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=1512295)\u001b[0m 2023-04-07 17:47:54,921\tWARNING env.py:296 -- Your MultiAgentEnv <GridWorldEnv instance> does not have some or all of the needed base-class attributes! Make sure you call `super().__init__()` from within your MutiAgentEnv's constructor. This will raise an error in the future.\n"
     ]
    }
   ],
   "source": [
    "from multiagent_env import GridWorldEnv\n",
    "from ray.rllib.algorithms.impala import ImpalaConfig, ImpalaTF1Policy, ImpalaTF2Policy, ImpalaTorchPolicy\n",
    "config = ImpalaConfig()\n",
    "config = config.training(lr=0.0003, train_batch_size=512)  \n",
    "config = config.resources(num_gpus=1)\n",
    "config = config.framework(\"torch\")  \n",
    "# config = config.rollouts(num_rollout_workers=8)  \n",
    "print(config.to_dict())  \n",
    "\n",
    "config = config.environment(env=GridWorldEnv)\n",
    "config = config.rollouts(num_rollout_workers=8)\n",
    "# config[\"exploration_config\"] = {\n",
    "#     \"type\": \"UCB\",\n",
    "#     \"ucb_coeff\": 2.0\n",
    "# }\n",
    "\n",
    "env = GridWorldEnv()\n",
    "policies = {\n",
    "    \"policy_1\": (ImpalaTF2Policy, env.observation_space, env.action_space, {}),\n",
    "}\n",
    "policy_mapping_fn = lambda agent_id, episode, worker, **kwargs: \"policy_1\"\n",
    "\n",
    "rllib_trainer = config.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration=1: R(\"return\")=269.8414634146341\n",
      "Iteration=2: R(\"return\")=115.78947368421052\n",
      "Iteration=3: R(\"return\")=-233.5664739884393\n",
      "Iteration=4: R(\"return\")=-270.49397590361446\n",
      "Iteration=5: R(\"return\")=-273.79761904761904\n",
      "Iteration=6: R(\"return\")=-179.32738095238096\n",
      "Iteration=7: R(\"return\")=-438.38235294117646\n",
      "Iteration=8: R(\"return\")=-434.03614457831327\n",
      "Iteration=9: R(\"return\")=-260.07100591715977\n",
      "Iteration=10: R(\"return\")=-247.04117647058823\n",
      "Iteration=11: R(\"return\")=-222.22222222222223\n",
      "Iteration=12: R(\"return\")=-159.66265060240963\n",
      "Iteration=13: R(\"return\")=-141.92261904761904\n",
      "Iteration=14: R(\"return\")=-164.19277108433735\n",
      "Iteration=15: R(\"return\")=-158.25766871165644\n",
      "Iteration=16: R(\"return\")=-128.16666666666666\n",
      "Iteration=17: R(\"return\")=-164.87951807228916\n",
      "Iteration=18: R(\"return\")=-133.52409638554218\n",
      "Iteration=19: R(\"return\")=-116.0\n",
      "Iteration=20: R(\"return\")=-132.8470588235294\n",
      "Iteration=21: R(\"return\")=-158.3012048192771\n",
      "Iteration=22: R(\"return\")=-192.67073170731706\n",
      "Iteration=23: R(\"return\")=-164.84242424242424\n",
      "Iteration=24: R(\"return\")=-158.9940828402367\n",
      "Iteration=25: R(\"return\")=-132.89820359281438\n",
      "Iteration=26: R(\"return\")=-158.83536585365854\n",
      "Iteration=27: R(\"return\")=-117.3030303030303\n",
      "Iteration=28: R(\"return\")=-108.59638554216868\n",
      "Iteration=29: R(\"return\")=-105.57232704402516\n",
      "Iteration=30: R(\"return\")=-103.07453416149069\n",
      "Iteration=31: R(\"return\")=-116.16167664670658\n",
      "Iteration=32: R(\"return\")=-124.79041916167665\n",
      "Iteration=33: R(\"return\")=-135.85\n",
      "Iteration=34: R(\"return\")=-142.6265060240964\n",
      "Iteration=35: R(\"return\")=-110.55757575757576\n",
      "Iteration=36: R(\"return\")=-99.36969696969697\n",
      "Iteration=37: R(\"return\")=-106.3167701863354\n",
      "Iteration=38: R(\"return\")=-101.01818181818182\n",
      "Iteration=39: R(\"return\")=-105.54878048780488\n",
      "Iteration=40: R(\"return\")=-102.45962732919254\n",
      "Iteration=41: R(\"return\")=-130.95705521472394\n",
      "Iteration=42: R(\"return\")=-134.86060606060607\n",
      "Iteration=43: R(\"return\")=-130.5421686746988\n",
      "Iteration=44: R(\"return\")=-120.25\n",
      "Iteration=45: R(\"return\")=-133.53374233128835\n",
      "Iteration=46: R(\"return\")=-116.76687116564418\n",
      "Iteration=47: R(\"return\")=-127.48447204968944\n",
      "Iteration=48: R(\"return\")=-144.24844720496895\n",
      "Iteration=49: R(\"return\")=-123.66257668711657\n",
      "Iteration=50: R(\"return\")=-115.18072289156626\n",
      "Iteration=51: R(\"return\")=-110.78481012658227\n",
      "Iteration=52: R(\"return\")=-109.11042944785277\n",
      "Iteration=53: R(\"return\")=-119.25454545454545\n",
      "Iteration=54: R(\"return\")=-119.81410256410257\n",
      "Iteration=55: R(\"return\")=-108.52597402597402\n",
      "Iteration=56: R(\"return\")=-101.55151515151515\n",
      "Iteration=57: R(\"return\")=-103.53658536585365\n",
      "Iteration=58: R(\"return\")=-103.67924528301887\n",
      "Iteration=59: R(\"return\")=-116.36075949367088\n",
      "Iteration=60: R(\"return\")=-127.2625\n",
      "Iteration=61: R(\"return\")=-133.1840490797546\n",
      "Iteration=62: R(\"return\")=-107.96855345911949\n",
      "Iteration=63: R(\"return\")=-105.52147239263803\n",
      "Iteration=64: R(\"return\")=-108.62893081761007\n",
      "Iteration=65: R(\"return\")=-100.0920245398773\n",
      "Iteration=66: R(\"return\")=-98.59235668789809\n",
      "Iteration=67: R(\"return\")=-98.4625\n",
      "Iteration=68: R(\"return\")=-100.0\n",
      "Iteration=69: R(\"return\")=-119.99371069182389\n",
      "Iteration=70: R(\"return\")=-115.9171974522293\n",
      "Iteration=71: R(\"return\")=-119.86792452830188\n",
      "Iteration=72: R(\"return\")=-115.3416149068323\n",
      "Iteration=73: R(\"return\")=-102.10191082802548\n",
      "Iteration=74: R(\"return\")=-108.85534591194968\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"model_checkpoint/best_model\"\n",
    "best_reward = 0\n",
    "for _ in range(1000):\n",
    "    results = rllib_trainer.train()\n",
    "    print(f\"Iteration={rllib_trainer.iteration}: R(\\\"return\\\")={results['episode_reward_mean']}\")\n",
    "    if results['episode_reward_mean'] > best_reward:\n",
    "        best_reward = results['episode_reward_mean']\n",
    "        rllib_trainer.save(checkpoint_path)\n",
    "    if results['episode_reward_mean'] > 1000:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "luxai_s2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
